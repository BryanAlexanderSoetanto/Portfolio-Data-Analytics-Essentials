import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import MinMaxScaler

%matplotlib inline

# Read the dataset
df = pd.read_csv("/content/TGM 2020-2023_eng.csv", sep=";")

# Print detected column names
print("=== Detected Columns ===")
print(df.columns.tolist(), "\n")

# Display the first few rows to verify the structure
print("=== First 5 Rows ===")
print(df.head(), "\n")

# Basic exploration: info, describe, missing values
print("=== DataFrame Info ===")
print(df.info(), "\n")
print("=== Descriptive Statistics ===")
print(df.describe(include='all'), "\n")
print("=== Missing Values Per Column ===")
print(df.isnull().sum(), "\n")

# Remove duplicate rows (if any)
duplicates = df.duplicated().sum()
print("Number of duplicate rows:", duplicates)
if duplicates > 0:
    df = df.drop_duplicates()
    print("After removing duplicates, shape:", df.shape, "\n")
else:
    print("No duplicate rows found.\n")

# Handle missing values and convert columns that contain commas to proper numeric
reading_col = "Tingkat Kegemaran Membaca (Reading Interest)"

numeric_cols_with_commas = [
    "Reading Frequency per week",
    "Number of Readings per Quarter",
    "Daily Reading Duration (in minutes)",
    "Internet Access Frequency per Week",
    "Daily Internet Duration (in minutes)",
    reading_col
]

for col in numeric_cols_with_commas:
    if col in df.columns:
        df[col] = df[col].astype(str).str.replace(',', '.')
        df[col] = pd.to_numeric(df[col], errors='coerce')
        median_val = df[col].median()
        df[col] = df[col].fillna(median_val)
        print(f"Missing values in '{col}' filled with median: {median_val}")

if 'Year' in df.columns:
    df['Year'] = pd.to_numeric(df['Year'], errors='coerce')
    year_median = df['Year'].median()
    df['Year'] = df['Year'].fillna(year_median)
    print(f"Missing values in 'Year' filled with median: {year_median}")

categorical_cols = ["Provinsi", "Category"]
for cat_col in categorical_cols:
    if cat_col in df.columns:
        mode_val = df[cat_col].mode()[0]
        df[cat_col] = df[cat_col].fillna(mode_val)
        print(f"Missing values in '{cat_col}' filled with mode: {mode_val}")
print()

# Visualize the reading interest BEFORE normalization
plt.figure(figsize=(8, 6))
sns.histplot(df[reading_col], bins=20, kde=True)
plt.title(f"Distribution of '{reading_col}' (Before Outlier Removal & Normalization)")
plt.xlabel(reading_col)
plt.ylabel("Frequency")
plt.show()

# Detect and remove outliers using the IQR method
def remove_outliers_iqr(data, cols):
    data_clean = data.copy()
    for c in cols:
        Q1 = data_clean[c].quantile(0.25)
        Q3 = data_clean[c].quantile(0.75)
        IQR = Q3 - Q1
        lower_bound = Q1 - 1.5 * IQR
        upper_bound = Q3 + 1.5 * IQR
        outlier_indices = data_clean[(data_clean[c] < lower_bound) | (data_clean[c] > upper_bound)].index
        print(f"Column '{c}': {len(outlier_indices)} outliers detected.")
        data_clean = data_clean.drop(index=outlier_indices)
    return data_clean

# Identify numeric columns for outlier detection
numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()
print("Numeric columns:", numeric_cols, "\n")

# Remove outliers only from 'Internet Access Frequency per Week' and 'Daily Internet Duration (in minutes)'
cols_to_remove_outliers = ["Internet Access Frequency per Week", "Daily Internet Duration (in minutes)"]
df_no_outliers_specific = remove_outliers_iqr(df, cols_to_remove_outliers)
print("\nShape after specific outlier removal:", df_no_outliers_specific.shape)

# Visualize the reading interest AFTER specific outlier removal
plt.figure(figsize=(8, 6))
sns.histplot(df_no_outliers_specific[reading_col], bins=20, kde=True, color='orange')
plt.title(f"Distribution of '{reading_col}' (After Outlier Removal from Internet-related Columns)")
plt.xlabel(reading_col)
plt.ylabel("Frequency")
plt.show()

# Normalize the data AFTER specific outlier removal using MinMaxScaler
df_no_outliers_specific_normalized = df_no_outliers_specific.copy()
scaler = MinMaxScaler()
# Re-identify numeric columns in the outlier-removed dataset
numeric_cols_specific_no_outliers = df_no_outliers_specific_normalized.select_dtypes(include=[np.number]).columns.tolist()
df_no_outliers_specific_normalized[numeric_cols_specific_no_outliers] = scaler.fit_transform(df_no_outliers_specific_normalized[numeric_cols_specific_no_outliers])

# Visualize the reading interest AFTER normalization
plt.figure(figsize=(8, 6))
sns.histplot(df_no_outliers_specific_normalized[reading_col], bins=20, kde=True, color='purple')
plt.title(f"Distribution of '{reading_col}' (After Normalization on Specifically Outlier-Free Data)")
plt.xlabel(f"{reading_col} (Normalized)")
plt.ylabel("Frequency")
plt.show()

print("\nSample of normalized data (after specific outlier removal):")
print(df_no_outliers_specific_normalized.head())

# Save the preprocessed datasets
df_no_outliers_specific_normalized.to_csv("TGM_2020-2023_normalized_specific_outliers.csv", index=False)
df_no_outliers_specific.to_csv("TGM_2020-2023_cleaned_specific_outliers.csv", index=False)
print("\nNormalized dataset (specific outlier-free) saved as 'TGM_2020-2023_normalized_specific_outliers.csv'.")
print("Cleaned (specific outliers removed) dataset saved as 'TGM_2020-2023_cleaned_specific_outliers.csv'.")
